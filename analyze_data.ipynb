{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, json, operator\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['All', 'Trump', 'NoTrump']\n",
    "train_data = list()\n",
    "train_labels = list()\n",
    "data_source = {'realDonaldTrump': 12322,\n",
    "             'SecretaryCarson': 1242,\n",
    "             'MartinOMalley': 2211,\n",
    "             'JebBush': 2757,\n",
    "             'BarackObama': 2880,\n",
    "             'HillaryClinton': 1774,\n",
    "             'BernieSanders': 1458}\n",
    "text_length = {key: defaultdict(int) for key in labels}\n",
    "word_level = {key: defaultdict(int) for key in labels}\n",
    "unigram_count = {key: defaultdict(int) for key in labels}\n",
    "bigram_count = {key: defaultdict(int) for key in labels}\n",
    "trigram_count = {key: defaultdict(int) for key in labels}\n",
    "dev_data = list()\n",
    "dev_labels = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect statistics\n",
    "TODO: word difficulty classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_label = 'All'\n",
    "phrases = ['uni', 'bi', 'tri']\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "Trump_vocab = {key: defaultdict(int) for key in phrases}\n",
    "noTrump_vocab = {key: defaultdict(int) for key in phrases}\n",
    "line_count = 0\n",
    "with open('train.csv', newline='', encoding=\"utf8\") as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')    \n",
    "    for row in csv_reader:\n",
    "        text = row[0]\n",
    "        label = row[1]\n",
    "        train_data.append(text)\n",
    "        train_labels.append(label)        \n",
    "        text_length[all_label][int(math.ceil(len(text) / 10.0)) * 10] += 1\n",
    "        text_length[label][int(math.ceil(len(text) / 10.0)) * 10] += 1                        \n",
    "        words = tokenizer.tokenize(row[0])\n",
    "        for unigram in words:\n",
    "            unigram_count[label][unigram] += 1\n",
    "            unigram_count[all_label][unigram] += 1\n",
    "        for bigram in list(nltk.bigrams(words)):\n",
    "            bigram_count[label][bigram] += 1\n",
    "            bigram_count[all_label][bigram] += 1\n",
    "        for trigram in list(nltk.trigrams(words)):\n",
    "            trigram_count[label][trigram] += 1\n",
    "            trigram_count[all_label][trigram] += 1\n",
    "            \n",
    "with open('dev.csv', newline='', encoding=\"utf8\") as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')    \n",
    "    for row in csv_reader:                                \n",
    "        dev_data.append(row[0])\n",
    "        dev_labels.append(row[1])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import itertools\n",
    "label_counter = {'total': len(train_data), 'validation': len(dev_data)}\n",
    "label_counter.update(Counter(train_labels).most_common())\n",
    "for k, v in text_length.items():\n",
    "    text_length[k] = {vk: vv for (vk, vv) in sorted(v.items())}\n",
    "for k, v in unigram_count.items():\n",
    "    unigram_count[k] = {vk: vv for (vk, vv) in sorted(v.items(), key=operator.itemgetter(1), reverse=True)}\n",
    "    unigram_count[k] = dict(itertools.islice(unigram_count[k].items(), 1000))\n",
    "for k, v in bigram_count.items():\n",
    "    bigram_count[k] = {' '.join(vk): vv for (vk, vv) in sorted(v.items(), key=operator.itemgetter(1), reverse=True)}\n",
    "    bigram_count[k] = dict(itertools.islice(bigram_count[k].items(), 1000))\n",
    "for k, v in trigram_count.items():\n",
    "    trigram_count[k] = {' '.join(vk): vv for (vk, vv) in sorted(v.items(), key=operator.itemgetter(1), reverse=True)}\n",
    "    trigram_count[k] = dict(itertools.islice(trigram_count[k].items(), 1000))\n",
    "basic_dict = {'count': label_counter, 'length': text_length, 'source': data_source,\n",
    "              'unigram': unigram_count, 'bigram': bigram_count, 'trigram': trigram_count }            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset_statistic.json', 'w') as fp:\n",
    "    json.dump(basic_dict, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameters Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class Vectorizer(Enum):\n",
    "    \"\"\"Methods for feature extraction\"\"\"\n",
    "    Count = 1\n",
    "    TfIdf = 2\n",
    "\n",
    "\n",
    "class Arguments():\n",
    "    \"\"\" Store arguments from command lines. \"\"\"\n",
    "\n",
    "    def __init__(self):        \n",
    "        self.vectorizer = Vectorizer.Count\n",
    "        self.token_pattern = r'(?u)\\b\\w\\w+\\b'\n",
    "        self.ngram = 1        \n",
    "        self.min_df = 0.0\n",
    "        self.max_df = 1.0\n",
    "        self.lowercase = False\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "trainy = le.transform(train_labels)\n",
    "devy = le.transform(dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "def extract_feature(args, train_data, dev_data):\n",
    "    \"\"\"Extract feature vectors from train data.\n",
    "    \n",
    "    Use CountVectorizer or TfidfVectorizer.\n",
    "    \"\"\"\n",
    "    if args.ngram > 1:\n",
    "        args.token_pattern = r'\\b\\w+\\b'\n",
    "    if args.vectorizer is Vectorizer.Count:\n",
    "        vect = CountVectorizer(lowercase=args.lowercase, ngram_range=(\n",
    "            1, args.ngram), token_pattern=args.token_pattern, min_df=args.min_df, max_df=args.max_df)\n",
    "    elif args.vectorizer is Vectorizer.TfIdf:\n",
    "        vect = TfidfVectorizer(lowercase=args.lowercase, ngram_range=(\n",
    "            1, args.ngram), token_pattern=args.token_pattern, min_df=args.min_df, max_df=args.max_df)\n",
    "    trainX = vect.fit_transform(train_data)    \n",
    "    devX = vect.transform(dev_data)\n",
    "    return trainX, devX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "def train(X, y):\n",
    "    \"\"\"Train a classifie r using the given training data.\n",
    "\n",
    "    Trains logistic regression on the input data with default parameters.\n",
    "    \"\"\"        \n",
    "    cls = LogisticRegression(random_state=0, solver='lbfgs', max_iter=10000)\n",
    "    cls.fit(X, y)\n",
    "    return cls\n",
    "def evaluate(X, yt, cls, name='data'):\n",
    "    \"\"\"Evaluated a classifier on the given labeled data using accuracy.\"\"\"    \n",
    "    yp = cls.predict(X)\n",
    "    acc = metrics.accuracy_score(yt, yp)\n",
    "    print(\"  Accuracy on %s  is: %s\" % (name, acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(train_data, dev_data, args):\n",
    "    \"\"\"Tune the hyper-parameters: n-gram, minimum count.\n",
    "\n",
    "    Use the approach of grid search.\n",
    "    \"\"\"    \n",
    "    best_acc = 0\n",
    "    best_f = Vectorizer.Count\n",
    "    best_n = 1\n",
    "    best_min = 0.0\n",
    "    best_max = 1.0\n",
    "    best_lowecase = False\n",
    "    with open(\"grid_search_2.csv\", \"w\", newline='') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        # Tune n-gram and minumum count\n",
    "        csv_writer.writerow(\n",
    "            ['Tokenizer', 'lowercase', 'N-gram', 'Min_df', 'Max_df', 'Accuracy'])\n",
    "        for f in Vectorizer:\n",
    "            args.vectorizer = f\n",
    "            for lowercase in [False, True]:\n",
    "                args.lowercase = lowercase\n",
    "                for n in range(1, 11):\n",
    "                    args.ngram = n\n",
    "                    for min_v in range(0, 11):\n",
    "                        args.min_df = min_v\n",
    "                        for max_v in range(10, 1210, 100):\n",
    "                            args.max_df = max_v\n",
    "                            trainX, devX = extract_feature(args, train_data, dev_data)\n",
    "                            cls = train(trainX, trainy)\n",
    "                            dev_acc = evaluate(devX, devy, cls)\n",
    "                            csv_writer.writerow([f, lowercase, n, min_v, max_v, dev_acc])\n",
    "                            if dev_acc > best_acc:\n",
    "                                best_lowercase = lowercase\n",
    "                                best_f = f\n",
    "                                best_acc = dev_acc\n",
    "                                best_n = n\n",
    "                                best_min = min_v\n",
    "                                best_max = max_v\n",
    "    args.vectorizer = best_f\n",
    "    args.min_df = best_min\n",
    "    args.ngram = best_n\n",
    "    args.max_df = best_max\n",
    "    args.lowercase = best_lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy on data  is: 0.9388264669163545\n",
      "  Accuracy on data  is: 0.9375780274656679\n",
      "  Accuracy on data  is: 0.9372659176029963\n",
      "  Accuracy on data  is: 0.9360174781523096\n",
      "  Accuracy on data  is: 0.9353932584269663\n",
      "  Accuracy on data  is: 0.9335205992509363\n",
      "  Accuracy on data  is: 0.9210362047440699\n",
      "  Accuracy on data  is: 0.9204119850187266\n",
      "  Accuracy on data  is: 0.9204119850187266\n",
      "  Accuracy on data  is: 0.9213483146067416\n",
      "  Accuracy on data  is: 0.9216604244694132\n",
      "  Accuracy on data  is: 0.9213483146067416\n",
      "  Accuracy on data  is: 0.9388264669163545\n"
     ]
    }
   ],
   "source": [
    "args = Arguments()\n",
    "grid_search(train_data, dev_data, args)\n",
    "trainX, devX = extract_feature(args, train_data, dev_data)\n",
    "cls = train(trainX, trainy)\n",
    "dev_acc = evaluate(devX, devy, cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_name = ['ngram', 'min', 'max']\n",
    "vects = ['Count', 'TF-IDF']\n",
    "para_dict = []\n",
    "with open('grid_search_typical.csv', newline='', encoding=\"utf8\") as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    vect = None\n",
    "    one_dict = {}\n",
    "    temp_dict = {}\n",
    "    current_name = ''\n",
    "    current_index = 0\n",
    "    for row in csv_reader:\n",
    "        if row[0] in vects:\n",
    "            if vect is None or row[0] != vect:\n",
    "                vect = row[0]\n",
    "                one_dict = {'Vectorizer': vect, 'lowercase': 'False'}                \n",
    "                para_dict.append(one_dict)\n",
    "            temp_dict = {}\n",
    "            current_name = row[1]\n",
    "            one_dict.update({row[1]: temp_dict})            \n",
    "        else:            \n",
    "            for i, name in enumerate(rows_name):\n",
    "                if name != current_name:\n",
    "                    if name not in temp_dict:\n",
    "                        temp_dict[name] = row[i + 2]\n",
    "                    else:\n",
    "                        break\n",
    "                elif name not in temp_dict:\n",
    "                    temp_dict[name] = {}\n",
    "                    current_index = i + 2\n",
    "            temp_dict[current_name][row[current_index]] = row[-1]                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hyper_parameters.json', 'w') as fp:\n",
    "    json.dump(para_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
