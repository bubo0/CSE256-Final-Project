{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, json, operator\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['All', 'Trump', 'NoTrump']\n",
    "train_data = list()\n",
    "train_labels = list()\n",
    "data_source = {'realDonaldTrump': 12322,\n",
    "             'SecretaryCarson': 1242,\n",
    "             'MartinOMalley': 2211,\n",
    "             'JebBush': 2757,\n",
    "             'BarackObama': 2880,\n",
    "             'HillaryClinton': 1774,\n",
    "             'BernieSanders': 1458}\n",
    "text_length = {key: defaultdict(int) for key in labels}\n",
    "word_level = {key: defaultdict(int) for key in labels}\n",
    "unigram_count = {key: defaultdict(int) for key in labels}\n",
    "bigram_count = {key: defaultdict(int) for key in labels}\n",
    "trigram_count = {key: defaultdict(int) for key in labels}\n",
    "dev_data = list()\n",
    "dev_labels = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect statistics\n",
    "TODO: word difficulty classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_label = 'All'\n",
    "phrases = ['uni', 'bi', 'tri']\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "Trump_vocab = {key: defaultdict(int) for key in phrases}\n",
    "noTrump_vocab = {key: defaultdict(int) for key in phrases}\n",
    "line_count = 0\n",
    "with open('train.csv', newline='', encoding=\"utf8\") as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')    \n",
    "    for row in csv_reader:\n",
    "        text = row[0]\n",
    "        label = row[1]\n",
    "        train_data.append(text)\n",
    "        train_labels.append(label)        \n",
    "        text_length[all_label][int(math.ceil(len(text) / 10.0)) * 10] += 1\n",
    "        text_length[label][int(math.ceil(len(text) / 10.0)) * 10] += 1                        \n",
    "        words = tokenizer.tokenize(row[0])\n",
    "        for unigram in words:\n",
    "            unigram_count[label][unigram] += 1\n",
    "            unigram_count[all_label][unigram] += 1\n",
    "        for bigram in list(nltk.bigrams(words)):\n",
    "            bigram_count[label][bigram] += 1\n",
    "            bigram_count[all_label][bigram] += 1\n",
    "        for trigram in list(nltk.trigrams(words)):\n",
    "            trigram_count[label][trigram] += 1\n",
    "            trigram_count[all_label][trigram] += 1\n",
    "            \n",
    "with open('dev.csv', newline='', encoding=\"utf8\") as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')    \n",
    "    for row in csv_reader:                                \n",
    "        dev_data.append(row[0])\n",
    "        dev_labels.append(row[1])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import itertools\n",
    "label_counter = {'total': len(train_data), 'validation': len(dev_data)}\n",
    "label_counter.update(Counter(train_labels).most_common())\n",
    "for k, v in text_length.items():\n",
    "    text_length[k] = {vk: vv for (vk, vv) in sorted(v.items())}\n",
    "for k, v in unigram_count.items():\n",
    "    unigram_count[k] = {vk: vv for (vk, vv) in sorted(v.items(), key=operator.itemgetter(1), reverse=True)}\n",
    "    unigram_count[k] = dict(itertools.islice(unigram_count[k].items(), 1000))\n",
    "for k, v in bigram_count.items():\n",
    "    bigram_count[k] = {' '.join(vk): vv for (vk, vv) in sorted(v.items(), key=operator.itemgetter(1), reverse=True)}\n",
    "    bigram_count[k] = dict(itertools.islice(bigram_count[k].items(), 1000))\n",
    "for k, v in trigram_count.items():\n",
    "    trigram_count[k] = {' '.join(vk): vv for (vk, vv) in sorted(v.items(), key=operator.itemgetter(1), reverse=True)}\n",
    "    trigram_count[k] = dict(itertools.islice(trigram_count[k].items(), 1000))\n",
    "basic_dict = {'count': label_counter, 'length': text_length, 'source': data_source,\n",
    "              'unigram': unigram_count, 'bigram': bigram_count, 'trigram': trigram_count }            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset_statistic.json', 'w') as fp:\n",
    "    json.dump(basic_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
